{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNCt7JL27E2zvixtJzjYB9A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elifsare/Titanic-Survival-Data/blob/main/Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABXxzpWb4OQD",
        "outputId": "970f9d81-fd4e-4f1b-e660-f0f0521a523c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "drive  MyDrive\toutput.csv  sample_data  weights.best.hdf5\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import re\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "3RmZxhCG5hsw"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "  data.Cabin.fillna('0', inplace=True)\n",
        "  data.loc[data.Cabin.str[0] == 'A', 'Cabin'] = 0\n",
        "  data.loc[data.Cabin.str[0] == 'B', 'Cabin'] = 1\n",
        "  data.loc[data.Cabin.str[0] == 'C', 'Cabin'] = 2\n",
        "  data.loc[data.Cabin.str[0] == 'D', 'Cabin'] = 3\n",
        "  data.loc[data.Cabin.str[0] == 'E', 'Cabin'] = 4\n",
        "  data.loc[data.Cabin.str[0] == 'F', 'Cabin'] = 5\n",
        "  data.loc[data.Cabin.str[0] == 'G', 'Cabin'] = 6\n",
        "  data.loc[data.Cabin.str[0] == 'T', 'Cabin'] = 7\n",
        "    \n",
        "  # Cinsiyeti tam sayıya çevirelim\n",
        "  data['Sex'].replace('female', 0, inplace=True)\n",
        "  data['Sex'].replace('male', 1, inplace=True)\n",
        "    \n",
        "  # Gemiye biniş limanlarını tam sayıya çevirelim\n",
        "  data['Embarked'].replace('S', 0, inplace=True)\n",
        "  data['Embarked'].replace('C', 1, inplace=True)\n",
        "  data['Embarked'].replace('Q', 2, inplace=True)\n",
        "    \n",
        "  # Olmayan (NA) yaş değerlerini medyan ile dolduralım\n",
        "  data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "  data['Fare'].fillna(data['Fare'].median(), inplace=True)\n",
        "  data['Embarked'].fillna(data['Embarked'].median(), inplace=True)\n",
        "    \n",
        "  # İstersek olmayan (NA) değerleri tamamen silebiliriz\n",
        "  # data.dropna(subset=['Fare', 'Embarked'], inplace=True, how='any')\n",
        "  return data\n",
        "\n"
      ],
      "metadata": {
        "id": "hpGxlJOe5hwH"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data içindeki gupların başlıklarıni değiştirme\n",
        "def group_titles(data):\n",
        "    data['Names'] = data['Name'].map(lambda x: len(re.split(' ', x)))\n",
        "    data['Title'] = data['Name'].map(lambda x: re.search(', (.+?) ', x).group(1))\n",
        "    data['Title'].replace('Master.', 0, inplace=True)\n",
        "    data['Title'].replace('Mr.', 1, inplace=True)\n",
        "    data['Title'].replace(['Ms.','Mlle.', 'Miss.'], 2, inplace=True)\n",
        "    data['Title'].replace(['Mme.', 'Mrs.'], 3, inplace=True)\n",
        "    data['Title'].replace(['Dona.', 'Lady.', 'the Countess.', 'Capt.', 'Col.', 'Don.', 'Dr.', 'Major.', 'Rev.', 'Sir.', 'Jonkheer.', 'the'], 4, inplace=True)"
      ],
      "metadata": {
        "id": "p2IQXZn25hzU"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_subset(data):\n",
        "    features = ['Pclass', 'SibSp', 'Parch', 'Sex', 'Names', 'Title', 'Age', 'Cabin'] #, 'Fare', 'Embarked']\n",
        "    lengh_features = len(features)\n",
        "    subset = data[features]#.fillna(0)\n",
        "    return subset, lengh_features\n"
      ],
      "metadata": {
        "id": "dRlOYyTX5h1p"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modelin içinde neler olacak, tasarım\n",
        "# Define Sequential model with 3 layers\n",
        "# Also, let's build a Sequential model incrementally via the add() method\n",
        "def create_model(train_set_size, input_length, num_epochs, batch_size):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(7, input_dim=input_length, activation='softplus'))\n",
        "  model.add(Dense(3, activation='softplus'))\n",
        "  model.add(Dense(1, activation='softplus'))\n",
        "\n",
        "  lr = .001 # Learning_rate, Defaults to 0.001.\n",
        "  adam0 = Adam(lr = lr)\n",
        "\n",
        "  # loss='binary_crossentropy'\n",
        "  # Entropy is a measure of the uncertainty associated with a given distribution q(y).\n",
        "  # Binary crossentropy is a loss function that is used in binary classification tasks. These are tasks that answer a question with only two choices. \n",
        "  #Several independent such questions can be answered at the same time, as in multi-label classification.\n",
        "  # Configures the model for training --> model.compile\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=adam0, metrics=['accuracy']) # metrik olarak doğruluk ölçümü\n",
        "\n",
        "  filepath = 'weights.best.hdf5' # öğrenilen ağırlığın kaydı için dosya\n",
        "\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True, mode='max') # checkpoint yardımıyla best result'ları belirler\n",
        "  callbacks_list = [checkpoint]\n",
        "  \n",
        "  history_model = model.fit(X_train[:train_set_size], Y_train[:train_set_size], callbacks=callbacks_list, epochs=num_epochs, batch_size=batch_size, verbose=0) #40, 32\n",
        "  return model, history_model"
      ],
      "metadata": {
        "id": "pL32-Zo55h4L"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plots(history):\n",
        "  loss_history = history.history['loss']\n",
        "  acc_history = history.history['accuracy']\n",
        "  epochs = [(i + 1) for i in range(num_epochs)]\n",
        "\n",
        "  ax = plt.subplot(211)\n",
        "  ax.plot(epochs, loss_history, color='red')\n",
        "  ax.set_xlabel('Epochs')\n",
        "  ax.set_ylabel('Error Rate\\n')\n",
        "  ax.set_title('Error Rate per Epoch\\n')\n",
        "\n",
        "  ax2 = plt.subplot(212)\n",
        "  ax2.plot(epochs, acc_history, color='blue')\n",
        "  ax2.set_xlabel('Epochs')\n",
        "  ax2.set_ylabel('Accuracy\\n')\n",
        "  ax2.set_title('Accuracy per Epoch\\n')\n",
        "\n",
        "  plt.subplots_adjust(hspace=0.8)\n",
        "  plt.savefig('Accuracy_loss.png')\n",
        "  plt.close()                                 "
      ],
      "metadata": {
        "id": "xWUCJ_oL5h6m"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(batch_size):\n",
        "    \n",
        "    test = pd.read_csv('/content/drive/MyDrive/Titanic/test.csv', header=0)\n",
        "    test_ids = test['PassengerId']\n",
        "    test = preprocess(test)\n",
        "    group_titles(test)\n",
        "    testdata, _ = data_subset(test)\n",
        "\n",
        "    X_test = np.array(testdata).astype(float)\n",
        "\n",
        "    output = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
        "    output = output.reshape((418,))\n",
        "\n",
        "    column_0 = np.concatenate((['PassengerId'], test_ids ), axis=0 )\n",
        "    column_1 = np.concatenate( ( ['Survived'], output ), axis=0 )\n",
        "\n",
        "    f = open(\"output.csv\", \"w\")\n",
        "    writer = csv.writer(f)\n",
        "    for i in range(len(column_0)):\n",
        "        writer.writerow( [column_0[i]] + [column_1[i]])\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "el5McPEz5h9K"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fonksiyonları da çağırarak programın bütün olacağı kısım"
      ],
      "metadata": {
        "id": "ZvqJu6Tp5iZ4"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "train = pd.read_csv('/content/drive/MyDrive/Titanic/train.csv', header=0)\n",
        "\n",
        "preprocess(train)\n",
        "group_titles(train)\n",
        "\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "train_data, lengh_features = data_subset(train)\n",
        "\n",
        "Y_train = np.array(train['Survived']).astype(int)\n",
        "X_train = np.array(train_data).astype(float)\n",
        "\n",
        "\n",
        "train_set_size = int(.67 * len(X_train))\n",
        "\n",
        "\n",
        "model, history_model = create_model(train_set_size, lengh_features, num_epochs, batch_size)\n",
        "\n",
        "plots(history_model)\n",
        "\n",
        "X_validation = X_train[train_set_size:]\n",
        "Y_validation = Y_train[train_set_size:]\n",
        "\n",
        "loss_and_metrics = model.evaluate(X_validation, Y_validation, batch_size=batch_size)\n",
        "print (\"loss_and_metrics\")\n",
        "\n",
        "test(batch_size)"
      ],
      "metadata": {
        "id": "TYLMBm1l5ica",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb4d3df2-a3c3-4924-8168-221cd2803d70"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: accuracy improved from -inf to 0.60738, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 2: accuracy did not improve from 0.60738\n",
            "\n",
            "Epoch 3: accuracy did not improve from 0.60738\n",
            "\n",
            "Epoch 4: accuracy did not improve from 0.60738\n",
            "\n",
            "Epoch 5: accuracy did not improve from 0.60738\n",
            "\n",
            "Epoch 6: accuracy did not improve from 0.60738\n",
            "\n",
            "Epoch 7: accuracy did not improve from 0.60738\n",
            "\n",
            "Epoch 8: accuracy did not improve from 0.60738\n",
            "\n",
            "Epoch 9: accuracy did not improve from 0.60738\n",
            "\n",
            "Epoch 10: accuracy improved from 0.60738 to 0.61577, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 11: accuracy did not improve from 0.61577\n",
            "\n",
            "Epoch 12: accuracy improved from 0.61577 to 0.62248, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 13: accuracy improved from 0.62248 to 0.62416, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 14: accuracy improved from 0.62416 to 0.63591, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 15: accuracy improved from 0.63591 to 0.64262, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 16: accuracy improved from 0.64262 to 0.65268, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 17: accuracy improved from 0.65268 to 0.66946, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 18: accuracy improved from 0.66946 to 0.68289, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 19: accuracy improved from 0.68289 to 0.68456, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 20: accuracy improved from 0.68456 to 0.69463, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 21: accuracy improved from 0.69463 to 0.69966, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 22: accuracy improved from 0.69966 to 0.70134, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 23: accuracy improved from 0.70134 to 0.72483, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 24: accuracy improved from 0.72483 to 0.72651, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 25: accuracy improved from 0.72651 to 0.73826, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 26: accuracy improved from 0.73826 to 0.73993, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 27: accuracy improved from 0.73993 to 0.74161, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 28: accuracy improved from 0.74161 to 0.74329, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 29: accuracy improved from 0.74329 to 0.75168, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 30: accuracy improved from 0.75168 to 0.75336, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 31: accuracy improved from 0.75336 to 0.76846, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 32: accuracy did not improve from 0.76846\n",
            "\n",
            "Epoch 33: accuracy did not improve from 0.76846\n",
            "\n",
            "Epoch 34: accuracy improved from 0.76846 to 0.77517, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 35: accuracy improved from 0.77517 to 0.78020, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 36: accuracy improved from 0.78020 to 0.78523, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 37: accuracy did not improve from 0.78523\n",
            "\n",
            "Epoch 38: accuracy did not improve from 0.78523\n",
            "\n",
            "Epoch 39: accuracy did not improve from 0.78523\n",
            "\n",
            "Epoch 40: accuracy did not improve from 0.78523\n",
            "\n",
            "Epoch 41: accuracy improved from 0.78523 to 0.78691, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 42: accuracy improved from 0.78691 to 0.79866, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 43: accuracy did not improve from 0.79866\n",
            "\n",
            "Epoch 44: accuracy did not improve from 0.79866\n",
            "\n",
            "Epoch 45: accuracy did not improve from 0.79866\n",
            "\n",
            "Epoch 46: accuracy did not improve from 0.79866\n",
            "\n",
            "Epoch 47: accuracy did not improve from 0.79866\n",
            "\n",
            "Epoch 48: accuracy did not improve from 0.79866\n",
            "\n",
            "Epoch 49: accuracy did not improve from 0.79866\n",
            "\n",
            "Epoch 50: accuracy did not improve from 0.79866\n",
            "\n",
            "Epoch 51: accuracy improved from 0.79866 to 0.80034, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 52: accuracy did not improve from 0.80034\n",
            "\n",
            "Epoch 53: accuracy did not improve from 0.80034\n",
            "\n",
            "Epoch 54: accuracy did not improve from 0.80034\n",
            "\n",
            "Epoch 55: accuracy improved from 0.80034 to 0.80369, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 56: accuracy did not improve from 0.80369\n",
            "\n",
            "Epoch 57: accuracy did not improve from 0.80369\n",
            "\n",
            "Epoch 58: accuracy did not improve from 0.80369\n",
            "\n",
            "Epoch 59: accuracy did not improve from 0.80369\n",
            "\n",
            "Epoch 60: accuracy improved from 0.80369 to 0.80537, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 61: accuracy improved from 0.80537 to 0.80705, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 62: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 63: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 64: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 65: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 66: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 67: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 68: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 69: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 70: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 71: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 72: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 73: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 74: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 75: accuracy improved from 0.80705 to 0.80872, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 76: accuracy did not improve from 0.80872\n",
            "\n",
            "Epoch 77: accuracy did not improve from 0.80872\n",
            "\n",
            "Epoch 78: accuracy did not improve from 0.80872\n",
            "\n",
            "Epoch 79: accuracy improved from 0.80872 to 0.81040, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 80: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 81: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 82: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 83: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 84: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 85: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 86: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 87: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 88: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 89: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 90: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 91: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 92: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 93: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 94: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 95: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 96: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 97: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 98: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 99: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 100: accuracy did not improve from 0.81040\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.4475 - accuracy: 0.8169\n",
            "loss_and_metrics\n"
          ]
        }
      ]
    }
  ]
}