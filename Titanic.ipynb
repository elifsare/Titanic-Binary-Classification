{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPM2pSAfDNTmdtOn9KnSPVT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elifsare/Titanic-Binary-Classification/blob/main/Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABXxzpWb4OQD",
        "outputId": "f2667108-bc27-4933-a94d-9f8986b95ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Accuracy_loss.png  drive  MyDrive  output.csv  sample_data  weights.best.hdf5\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import re\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "3RmZxhCG5hsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "  data.Cabin.fillna('0', inplace=True)\n",
        "  data.loc[data.Cabin.str[0] == 'A', 'Cabin'] = 0\n",
        "  data.loc[data.Cabin.str[0] == 'B', 'Cabin'] = 1\n",
        "  data.loc[data.Cabin.str[0] == 'C', 'Cabin'] = 2\n",
        "  data.loc[data.Cabin.str[0] == 'D', 'Cabin'] = 3\n",
        "  data.loc[data.Cabin.str[0] == 'E', 'Cabin'] = 4\n",
        "  data.loc[data.Cabin.str[0] == 'F', 'Cabin'] = 5\n",
        "  data.loc[data.Cabin.str[0] == 'G', 'Cabin'] = 6\n",
        "  data.loc[data.Cabin.str[0] == 'T', 'Cabin'] = 7\n",
        "    \n",
        "  # Cinsiyeti tam sayıya çevirelim\n",
        "  data['Sex'].replace('female', 0, inplace=True)\n",
        "  data['Sex'].replace('male', 1, inplace=True)\n",
        "    \n",
        "  # Gemiye biniş limanlarını tam sayıya çevirelim\n",
        "  data['Embarked'].replace('S', 0, inplace=True)\n",
        "  data['Embarked'].replace('C', 1, inplace=True)\n",
        "  data['Embarked'].replace('Q', 2, inplace=True)\n",
        "    \n",
        "  # Olmayan (NA) yaş değerlerini medyan ile dolduralım\n",
        "  data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "  data['Fare'].fillna(data['Fare'].median(), inplace=True)\n",
        "  data['Embarked'].fillna(data['Embarked'].median(), inplace=True)\n",
        "    \n",
        "  # İstersek olmayan (NA) değerleri tamamen silebiliriz\n",
        "  # data.dropna(subset=['Fare', 'Embarked'], inplace=True, how='any')\n",
        "  return data\n",
        "\n"
      ],
      "metadata": {
        "id": "hpGxlJOe5hwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data içindeki gupların başlıklarıni değiştirme\n",
        "def group_titles(data):\n",
        "  data['Names'] = data['Name'].map(lambda x: len(re.split(' ', x)))\n",
        "  data['Title'] = data['Name'].map(lambda x: re.search(', (.+?) ', x).group(1))\n",
        "  data['Title'].replace('Master.', 0, inplace=True)\n",
        "  data['Title'].replace('Mr.', 1, inplace=True)\n",
        "  data['Title'].replace(['Ms.','Mlle.', 'Miss.'], 2, inplace=True)\n",
        "  data['Title'].replace(['Mme.', 'Mrs.'], 3, inplace=True)\n",
        "  data['Title'].replace(['Dona.', 'Lady.', 'the Countess.', 'Capt.', 'Col.', 'Don.', 'Dr.', 'Major.', 'Rev.', 'Sir.', 'Jonkheer.', 'the'], 4, inplace=True)"
      ],
      "metadata": {
        "id": "p2IQXZn25hzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_subset(data):\n",
        "  features = ['Pclass', 'SibSp', 'Parch', 'Sex', 'Names', 'Title', 'Age', 'Cabin'] #, 'Fare', 'Embarked']\n",
        "  lengh_features = len(features)\n",
        "  subset = data[features]#.fillna(0)\n",
        "  return subset, lengh_features\n"
      ],
      "metadata": {
        "id": "dRlOYyTX5h1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modelin içinde neler olacak, tasarım\n",
        "# Define Sequential model with 3 layers\n",
        "# Also, let's build a Sequential model incrementally via the add() method\n",
        "def create_model(train_set_size, input_length, num_epochs, batch_size):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(7, input_dim=input_length, activation='softplus'))\n",
        "  model.add(Dense(3, activation='softplus'))\n",
        "  model.add(Dense(1, activation='softplus'))\n",
        "\n",
        "  lr = .001 # Learning_rate, Defaults to 0.001.\n",
        "  adam0 = Adam(lr = lr)\n",
        "\n",
        "  # loss='binary_crossentropy'\n",
        "  # Entropy is a measure of the uncertainty associated with a given distribution q(y).\n",
        "  # Binary crossentropy is a loss function that is used in binary classification tasks. These are tasks that answer a question with only two choices. \n",
        "  #Several independent such questions can be answered at the same time, as in multi-label classification.\n",
        "  # Configures the model for training --> model.compile\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=adam0, metrics=['accuracy']) # metrik olarak doğruluk ölçümü\n",
        "\n",
        "  filepath = 'weights.best.hdf5' # öğrenilen ağırlığın kaydı için dosya\n",
        "\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True, mode='max') # checkpoint yardımıyla best result'ları belirler\n",
        "  callbacks_list = [checkpoint]\n",
        "  \n",
        "  history_model = model.fit(X_train[:train_set_size], Y_train[:train_set_size], callbacks=callbacks_list, epochs=num_epochs, batch_size=batch_size, verbose=0) #40, 32\n",
        "  return model, history_model"
      ],
      "metadata": {
        "id": "pL32-Zo55h4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plots(history):\n",
        "  loss_history = history.history['loss']\n",
        "  acc_history = history.history['accuracy']\n",
        "  epochs = [(i + 1) for i in range(num_epochs)]\n",
        "\n",
        "  ax = plt.subplot(211)\n",
        "  ax.plot(epochs, loss_history, color='red')\n",
        "  ax.set_xlabel('Epochs')\n",
        "  ax.set_ylabel('Error Rate\\n')\n",
        "  ax.set_title('Error Rate per Epoch\\n')\n",
        "\n",
        "  ax2 = plt.subplot(212)\n",
        "  ax2.plot(epochs, acc_history, color='blue')\n",
        "  ax2.set_xlabel('Epochs')\n",
        "  ax2.set_ylabel('Accuracy\\n')\n",
        "  ax2.set_title('Accuracy per Epoch\\n')\n",
        "\n",
        "  plt.subplots_adjust(hspace=0.8)\n",
        "  plt.savefig('Accuracy_loss.png')\n",
        "  plt.close()                                 "
      ],
      "metadata": {
        "id": "xWUCJ_oL5h6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(batch_size):\n",
        "    \n",
        "  test = pd.read_csv('/content/drive/MyDrive/Titanic/test.csv', header=0)\n",
        "  test_ids = test['PassengerId']\n",
        "  test = preprocess(test)\n",
        "  group_titles(test)\n",
        "  testdata, _ = data_subset(test)\n",
        "\n",
        "  X_test = np.array(testdata).astype(float)\n",
        "\n",
        "  output = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
        "  output = output.reshape((418,))\n",
        "\n",
        "  column_0 = np.concatenate((['PassengerId'], test_ids ), axis=0 )\n",
        "  column_1 = np.concatenate( ( ['Survived'], output ), axis=0 )\n",
        "\n",
        "  f = open(\"output.csv\", \"w\")\n",
        "  writer = csv.writer(f)\n",
        "  for i in range(len(column_0)):\n",
        "    writer.writerow( [column_0[i]] + [column_1[i]])\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "el5McPEz5h9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fonksiyonları da çağırarak programın bütün olacağı kısım"
      ],
      "metadata": {
        "id": "ZvqJu6Tp5iZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "train = pd.read_csv('/content/drive/MyDrive/Titanic/train.csv', header=0)\n",
        "\n",
        "preprocess(train)\n",
        "group_titles(train)\n",
        "\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "train_data, lengh_features = data_subset(train)\n",
        "\n",
        "Y_train = np.array(train['Survived']).astype(int)\n",
        "X_train = np.array(train_data).astype(float)\n",
        "\n",
        "\n",
        "train_set_size = int(.67 * len(X_train))\n",
        "\n",
        "\n",
        "model, history_model = create_model(train_set_size, lengh_features, num_epochs, batch_size)\n",
        "\n",
        "plots(history_model)\n",
        "\n",
        "X_validation = X_train[train_set_size:]\n",
        "Y_validation = Y_train[train_set_size:]\n",
        "\n",
        "loss_and_metrics = model.evaluate(X_validation, Y_validation, batch_size=batch_size)\n",
        "print (\"loss_and_metrics\")\n",
        "\n",
        "test(batch_size)"
      ],
      "metadata": {
        "id": "TYLMBm1l5ica",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d52b91d-b00b-4ac6-f20c-af88c393fae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: accuracy improved from -inf to 0.60738, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 2: accuracy did not improve from 0.60738\n",
            "\n",
            "Epoch 3: accuracy improved from 0.60738 to 0.61242, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 4: accuracy did not improve from 0.61242\n",
            "\n",
            "Epoch 5: accuracy did not improve from 0.61242\n",
            "\n",
            "Epoch 6: accuracy did not improve from 0.61242\n",
            "\n",
            "Epoch 7: accuracy improved from 0.61242 to 0.63423, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 8: accuracy improved from 0.63423 to 0.65940, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 9: accuracy improved from 0.65940 to 0.68624, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 10: accuracy did not improve from 0.68624\n",
            "\n",
            "Epoch 11: accuracy did not improve from 0.68624\n",
            "\n",
            "Epoch 12: accuracy did not improve from 0.68624\n",
            "\n",
            "Epoch 13: accuracy improved from 0.68624 to 0.68960, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 14: accuracy did not improve from 0.68960\n",
            "\n",
            "Epoch 15: accuracy did not improve from 0.68960\n",
            "\n",
            "Epoch 16: accuracy did not improve from 0.68960\n",
            "\n",
            "Epoch 17: accuracy did not improve from 0.68960\n",
            "\n",
            "Epoch 18: accuracy did not improve from 0.68960\n",
            "\n",
            "Epoch 19: accuracy did not improve from 0.68960\n",
            "\n",
            "Epoch 20: accuracy did not improve from 0.68960\n",
            "\n",
            "Epoch 21: accuracy did not improve from 0.68960\n",
            "\n",
            "Epoch 22: accuracy improved from 0.68960 to 0.69799, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 23: accuracy improved from 0.69799 to 0.70638, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 24: accuracy improved from 0.70638 to 0.72148, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 25: accuracy improved from 0.72148 to 0.73154, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 26: accuracy improved from 0.73154 to 0.74329, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 27: accuracy did not improve from 0.74329\n",
            "\n",
            "Epoch 28: accuracy did not improve from 0.74329\n",
            "\n",
            "Epoch 29: accuracy improved from 0.74329 to 0.74832, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 30: accuracy improved from 0.74832 to 0.75000, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 31: accuracy did not improve from 0.75000\n",
            "\n",
            "Epoch 32: accuracy improved from 0.75000 to 0.76174, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 33: accuracy improved from 0.76174 to 0.76342, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 34: accuracy improved from 0.76342 to 0.76510, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 35: accuracy improved from 0.76510 to 0.77181, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 36: accuracy did not improve from 0.77181\n",
            "\n",
            "Epoch 37: accuracy did not improve from 0.77181\n",
            "\n",
            "Epoch 38: accuracy did not improve from 0.77181\n",
            "\n",
            "Epoch 39: accuracy improved from 0.77181 to 0.77685, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 40: accuracy improved from 0.77685 to 0.78356, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 41: accuracy improved from 0.78356 to 0.78523, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 42: accuracy did not improve from 0.78523\n",
            "\n",
            "Epoch 43: accuracy did not improve from 0.78523\n",
            "\n",
            "Epoch 44: accuracy did not improve from 0.78523\n",
            "\n",
            "Epoch 45: accuracy improved from 0.78523 to 0.78691, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 46: accuracy improved from 0.78691 to 0.78859, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 47: accuracy improved from 0.78859 to 0.79027, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 48: accuracy did not improve from 0.79027\n",
            "\n",
            "Epoch 49: accuracy did not improve from 0.79027\n",
            "\n",
            "Epoch 50: accuracy improved from 0.79027 to 0.79362, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 51: accuracy improved from 0.79362 to 0.79698, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 52: accuracy did not improve from 0.79698\n",
            "\n",
            "Epoch 53: accuracy did not improve from 0.79698\n",
            "\n",
            "Epoch 54: accuracy did not improve from 0.79698\n",
            "\n",
            "Epoch 55: accuracy did not improve from 0.79698\n",
            "\n",
            "Epoch 56: accuracy did not improve from 0.79698\n",
            "\n",
            "Epoch 57: accuracy improved from 0.79698 to 0.79866, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 58: accuracy improved from 0.79866 to 0.80705, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 59: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 60: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 61: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 62: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 63: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 64: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 65: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 66: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 67: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 68: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 69: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 70: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 71: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 72: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 73: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 74: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 75: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 76: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 77: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 78: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 79: accuracy did not improve from 0.80705\n",
            "\n",
            "Epoch 80: accuracy improved from 0.80705 to 0.80872, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 81: accuracy did not improve from 0.80872\n",
            "\n",
            "Epoch 82: accuracy did not improve from 0.80872\n",
            "\n",
            "Epoch 83: accuracy improved from 0.80872 to 0.81040, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 84: accuracy did not improve from 0.81040\n",
            "\n",
            "Epoch 85: accuracy improved from 0.81040 to 0.81208, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 86: accuracy did not improve from 0.81208\n",
            "\n",
            "Epoch 87: accuracy did not improve from 0.81208\n",
            "\n",
            "Epoch 88: accuracy did not improve from 0.81208\n",
            "\n",
            "Epoch 89: accuracy did not improve from 0.81208\n",
            "\n",
            "Epoch 90: accuracy did not improve from 0.81208\n",
            "\n",
            "Epoch 91: accuracy did not improve from 0.81208\n",
            "\n",
            "Epoch 92: accuracy did not improve from 0.81208\n",
            "\n",
            "Epoch 93: accuracy did not improve from 0.81208\n",
            "\n",
            "Epoch 94: accuracy did not improve from 0.81208\n",
            "\n",
            "Epoch 95: accuracy did not improve from 0.81208\n",
            "\n",
            "Epoch 96: accuracy did not improve from 0.81208\n",
            "\n",
            "Epoch 97: accuracy did not improve from 0.81208\n",
            "\n",
            "Epoch 98: accuracy did not improve from 0.81208\n",
            "\n",
            "Epoch 99: accuracy did not improve from 0.81208\n",
            "\n",
            "Epoch 100: accuracy did not improve from 0.81208\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4424 - accuracy: 0.8000\n",
            "loss_and_metrics\n"
          ]
        }
      ]
    }
  ]
}